{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is deep learning?\n",
    "\n",
    "- Subcase of machine learning, which is itself a subcase of artificial intelligence\n",
    "- Artificial intelligence is about understanding concepts\n",
    "- Machine learning is about learning concepts through example\n",
    "- Deep learning is a specific machine learning method which works (very) well in some cases "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning tasks\n",
    "\n",
    "In machine learning, we teach a learning algorithm to learn to perform a task by showing it examples. We want the learning algorithm to find general patterns in what it is shown, and not to memorize. We typically represent an example with a vector $x$. If we're doing supervised learning then there will also be an accompanying label $y$. Here are a few common tasks:\n",
    "\n",
    "- **Classification**: the algorithm has to predict one of $k$ possible classes. Binary classification is special case where $k = 1$. In multi-output classification, there can be more than one class.\n",
    "- **Regression**: the algorithm has to predict a real-valued output. In multi-output regression, there can be multiple real-valued outputs to predict.\n",
    "- **Denoising**: the algorithm takes as input a noisy vector $\\tilde{x}$, and has to produce a clean output vector $x$. \n",
    "- **Auto-encoding**: the algorithm takes as input $x$, and has to predict the exact same output $x$. It is however constrained by it's capacity. If the algorithm doesn't manage to reproduce an example, then that can be indicator that the input is an outlier/anomaly with respect to the training data.\n",
    "- **Density estimation**: the algorithm has to output the density $p$ of a vector $x$. In effect the learning algorithm acts as a non-parametric probability distribution that fits the data.\n",
    "- **Optimal control**: The algorithm inputs a state $s$ of the *dynamic system/environment* and has to output an action $a$ then apply it in the *dynamic system/environment* (Deep reinforcement learning). \n",
    "- **Generative models**: Generate data belonging to the training data distribution $p_{x}$, starting from a vector (usually random) $z$. GANs or diffusion models are generative models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yet another explanation of backprop\n",
    "\n",
    "There are many tutorials on backpropagation out there. I've skimmed through a bunch of them, and overall my favorite was [this one](https://www.ritchievink.com/blog/2017/07/10/programming-a-neural-network-from-scratch/) by Ritchie Vink. I preferred because the code examples are of good quality and give a lot of leeway for improvement. [This](https://victorzhou.com/blog/intro-to-neural-networks/) blogpost by Victor Zhou also helped me develop a mental model of what's going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks in a nutshell\n",
    "\n",
    "A neural network is a sequence of layers. Every layer takes as input $x$ and outputs $z$. We can denote this by a function which we call $f$:\n",
    "\n",
    "$$z = f(x)$$\n",
    "\n",
    "Note that the input $x$ can be a set of features, as well as the output from another layer. In the case of a dense layer, $f$ is an affine transformation:\n",
    "\n",
    "$$z = w x + b$$\n",
    "\n",
    "When we stack layers, we are simply chaining functions:\n",
    "\n",
    "$$\\hat{y} = f(f(f(\\dots(f(x)))))$$\n",
    "\n",
    "In the case of dense layers, which are linear, chaining them essentially results in a linear function. This means that even if we have a million dense layers stacked together, we still won't be able to learn non-linear patterns such as the XOR function. To add non-linearity, we add an *activation function* after each layer. Let's call these activation functions $g$. The output from the activation functions will be called $a$.\n",
    "\n",
    "$$a = g(f(x))$$\n",
    "\n",
    "When we stack layers, our final output is:\n",
    "\n",
    "$$\\hat{y} = g(f(g(f(\\dots(g(f(x)))))))$$\n",
    "\n",
    "Of course there are many more flavors of neural networks but that's the general idea. In the case of using dense layers, we're looking to tune the weights $w$ and biases $b$. That's where backpropagation comes in.\n",
    "\n",
    "## Loss functions\n",
    "A loss function is a function that takes as input the output of the network $\\hat{y}$ and the ground truth $y$ and outputs a scalar value. The goal of the loss function is to indicate how far off the network is from the ground truth. The learning algorithm will then try to minimize the loss function. Here are a few examples of loss functions:\n",
    "\n",
    "**Mean Squared Error (MSE)**\n",
    "\n",
    "Used for general purpose regression. Penalizes large mistakes.\n",
    "\n",
    "$$L(y, \\hat{y}) = (y - \\hat{y}) ^ 2$$\n",
    "\n",
    "**Logistic loss**\n",
    "\n",
    "The most common loss used for classification is the cross-entropy loss. It's also called the logistic loss. It's used for binary classification, but can be extended to multi-class classification. It's also used for estimating probabilities.\n",
    "\n",
    "$$L(y, p) = log(1 + exp(-yp))$$\n",
    "\n",
    "**Poisson loss**\n",
    "\n",
    "Used for estimating counts (arrivals in an airport, number of call events to call center, etc.), which is a specific case of regression.\n",
    "\n",
    "$$L(y, \\hat{y}) = \\hat{y} - y \\times log(\\hat{y})$$\n",
    "\n",
    "**Hinge loss**\n",
    "\n",
    "$$L(y, p) = max(0, 1 - yp)$$\n",
    "\n",
    "Loss functions have a big impact on the learning algorithm. For instance the only difference between linear regression and logistic regression is that linear regression is a linear model with a squared loss whereas logistic regression is a linear model with a logistic loss.\n",
    "\n",
    "There are many more loss functions that you can use in machine learning. You can even design your own! For example the deep learning community introduced the [focal loss](https://arxiv.org/abs/1708.02002) to deal with imbalanced datasets. Vowpal Wabbit also [designed](https://arxiv.org/abs/1011.1576) a set of loss functions that support importance weights.\n",
    "\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "First of all, let's get the chain rule out of the way. Say you have a function $f$, a function $g$, and an input $x$. If we compose our functions and apply them to $x$ we get $g(f(x))$. Now say we want to find the derivative of $g$ with respect to $x$. The trick is that there the function $f$ in between $g$ and $x$. In this case we use the chain rule, which gives us:\n",
    "\n",
    "$$\\frac{\\partial g}{\\partial x} = \\frac{\\partial g}{\\partial f} \\times \\frac{\\partial f}{\\partial x}$$\n",
    "\n",
    "In other words, in order to compute $\\frac{\\partial g}{\\partial x}$, we have to compute $\\frac{\\partial g}{\\partial f}$ and $\\frac{\\partial f}{\\partial x}$ and multiply them together. The chain rule is thus just a tool that we can add to our toolkit. In the case of neural networks it's super useful because we're basically just chaining functions. \n",
    "\n",
    "Let's say we're looking at the weights of the final layer. We'll call them $w$. The output of the network is denoted as $\\hat{y}$ whilst the ground truth is $y$. We have a loss function $L$ which indicates the error between $y$ and $\\hat{y}$. To update the weights, we need to calculate the gradient of the loss function with respect to the weights:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w}$$\n",
    "\n",
    "In between $w_i$ and $L$, there is the application of the dense layer and the activation function. We can thus apply the chain rule:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial a} \\times \\frac{\\partial a}{\\partial z} \\times \\frac{\\partial z}{\\partial w}$$\n",
    "\n",
    "In the case where our loss function is the mean squared error, the derivative is:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial a} = 2 \\times (a - y)$$\n",
    "\n",
    "For a sigmoid activation function, the derivative is:\n",
    "\n",
    "$$\\frac{\\partial a}{\\partial z} = \\sigma(z) (1 - \\sigma(z))$$\n",
    "\n",
    "where $\\sigma$ is in fact the sigmoid function. In the case of a dense layer, the derivative is:\n",
    "\n",
    "$$\\frac{\\partial z}{\\partial w} = x$$\n",
    "\n",
    "We simply have to multiply all these elements together in order to obtain $\\frac{\\partial L}{\\partial w}$:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w} = (2 \\times (a - y)) \\times (\\sigma(z) (1 - \\sigma(z))) \\times x$$\n",
    "\n",
    "Recall that $a$ is the output of the network after having been processed by the activation function. We could have as well called it $\\hat{y}$ because we're looking at the final layer, but we use $a$ because it's more generic and applies to each layer in the network. $z$ is the output of the network *before* being processed by the activation function. Note that implementation wise we thus have to keep both in memory. We can't just obtain $a$ and erase $z$.\n",
    "\n",
    "If we plug in a different activation function and/or a different loss function, then everything will still work as long as each element is differentiable. Note that if we use the identity activation function (which doesn't change the input and has a derivative of 1), then we're simply doing linear regression!\n",
    "\n",
    "Now how about the weights of the penultimate layer (the one just before the last one). Well we \"just\" have write it down using the chain rule. Here goes:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_2} = \\frac{\\partial L}{\\partial a_3} \\times \\frac{\\partial a_3}{\\partial z_3} \\times \\frac{\\partial z_3}{\\partial a_2} \\times \\frac{\\partial a_2}{\\partial z_2} \\times \\frac{\\partial z_2}{\\partial w_2}$$\n",
    "\n",
    "We've indexed the $a$s and $z$s because we're looking at multiple layer. In this case $a_3$ is the output of the 3rd layer (we called it $a$ before) whilst $a_2$ is the output of the 2nd layer. An important thing to notice is that we're using $\\frac{\\partial L}{\\partial a_3} \\times \\frac{\\partial a_3}{\\partial z_3}$, which we already calculated previously. We can exploit this when we implement backpropagation in order to speed up our code but also make it shorter.\n",
    "\n",
    "Here is the gradients for the weights of the 1st layer:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_2} = \\frac{\\partial L}{\\partial a_3} \\times \\frac{\\partial a_3}{\\partial z_3} \\times \\frac{\\partial z_3}{\\partial a_2} \\times \\frac{\\partial a_2}{\\partial z_2} \\times \\frac{\\partial z_2}{\\partial a_1} \\times \\frac{\\partial a_1}{\\partial z_1} \\times \\frac{\\partial z_1}{\\partial w_1}$$\n",
    "\n",
    "Again the first four elements of the product have already been computed.\n",
    "\n",
    "How about the biases $b_i$? Well in a dense layer the derivative with respect to the biases is 1 (it was $x$ with respect to the weights). For the 3rd layer this will result in:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b} = (2 \\times (a - y)) \\times (\\sigma(z) (1 - \\sigma(z))) \\times 1$$\n",
    "\n",
    "## Stochastic gradient descent\n",
    "\n",
    "1. For each observation $(x_i, y_i)$, the gradient $\\nabla_i$ is obtained\n",
    "2. An optimizer takes care of obtaining the new weights $w_{i+1}$ by modifying the current weights $w_i$ and using the current gradient $\\nabla_i$\n",
    "3. We can loop multiple times through the dataset; each iteration is called an **epoch**\n",
    "\n",
    "A general formulation of stochastic gradient descent (SGD):\n",
    "\n",
    "$$w_{i+1} \\leftarrow f(w_i, \\nabla_i, \\eta_i)$$\n",
    "\n",
    "$\\eta_i$ is the learning rate at iteration $i$, it's *extremely* important and we'll come back to it very soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 0: </b>  \n",
    "In your words comment the following code. Explain what is happening at each step.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "class ReLU:\n",
    "    \"\"\"Rectified Linear Unit (ReLU) activation function.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def activation(z):\n",
    "        z[z < 0] = 0\n",
    "        return z\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient(z):\n",
    "        z[z < 0] = 0\n",
    "        z[z > 0] = 1\n",
    "        return z\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def activation(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient(z):\n",
    "        s = Sigmoid.activation(z)\n",
    "        return s * (1 - s)\n",
    "\n",
    "\n",
    "class Identity:\n",
    "    \"\"\"Identity activation function.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def activation(z):\n",
    "        return z\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient(z):\n",
    "        return np.ones_like(z)\n",
    "\n",
    "\n",
    "class MSE:\n",
    "    \"\"\"Mean Squared Error (MSE) loss function.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def loss(y_true, y_pred):\n",
    "        return np.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient(y_true, y_pred):\n",
    "        return 2 * (y_pred - y_true)\n",
    "\n",
    "\n",
    "class SGD:\n",
    "    \"\"\"Stochastic Gradient Descent (SGD).\"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def step(self, weights, gradients):\n",
    "        weights -= self.learning_rate * gradients\n",
    "\n",
    "\n",
    "class NN:\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters:\n",
    "        dimensions (tuples of ints of length n_layers)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dimensions, activations, loss, optimizer):\n",
    "        self.n_layers = len(dimensions)\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        # Weights and biases are initiated by index. For a one hidden layer net you will have a w[1] and w[2]\n",
    "        self.w = {}\n",
    "        self.b = {}\n",
    "\n",
    "        # Activations are also initiated by index. For the example we will have activations[2] and activations[3]\n",
    "        self.activations = {}\n",
    "        for i in range(len(dimensions) - 1):\n",
    "            self.w[i + 1] = np.random.randn(dimensions[i], dimensions[i + 1]) / np.sqrt(dimensions[i])\n",
    "            self.b[i + 1] = np.zeros(dimensions[i + 1])\n",
    "            self.activations[i + 2] = activations[i]\n",
    "\n",
    "    def _feed_forward(self, X):\n",
    "        \"\"\"Executes a forward pass through the neural network.\n",
    "\n",
    "        This will return the state at each layer of the network, which includes the output of the\n",
    "        network.\n",
    "\n",
    "        Parameters:\n",
    "            X (array of shape (batch_size, n_features))\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # z = w(x) + b\n",
    "        z = {}\n",
    "\n",
    "        # a = f(z)\n",
    "        a = {1: X}  # First layer has no activations as input\n",
    "\n",
    "        for i in range(2, self.n_layers + 1):\n",
    "            z[i] = np.dot(a[i - 1], self.w[i - 1]) + self.b[i - 1]\n",
    "            a[i] = self.activations[i].activation(z[i])\n",
    "\n",
    "        return z, a\n",
    "\n",
    "    def _backprop(self, z, a, y_true):\n",
    "        \"\"\"Backpropagation.\n",
    "\n",
    "        Parameters:\n",
    "            z (dict of length n_layers - 1):\n",
    "\n",
    "                z = {\n",
    "                    2: w1 * x + b1\n",
    "                    3: w2 * (w1 * x + b1) + b2\n",
    "                    4: w3 * (w2 * (w1 * x + b1) + b2) + b3\n",
    "                    ...\n",
    "                }\n",
    "\n",
    "            a (dict of length n_layers):\n",
    "\n",
    "                a = {\n",
    "                    1: x,\n",
    "                    2: f(w1 * x + b1)\n",
    "                    3: f(w2 * (w1 * x + b1) + b2)\n",
    "                    4: f(w3 * (w2 * (w1 * x + b1) + b2) + b3)\n",
    "                    ...\n",
    "                }\n",
    "\n",
    "            y_true (array of shape (batch_size, n_targets))\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Determine the partial derivative and delta for the output layer\n",
    "        y_pred = a[self.n_layers]\n",
    "        final_activation = self.activations[self.n_layers]\n",
    "        delta = self.loss.gradient(y_true, y_pred) * final_activation.gradient(y_pred)\n",
    "        dw = np.dot(a[self.n_layers - 1].T, delta)\n",
    "\n",
    "        update_params = {\n",
    "            self.n_layers - 1: (dw, delta)\n",
    "        }\n",
    "\n",
    "        # Go through the layers in reverse order\n",
    "        for i in range(self.n_layers - 2, 0, -1):\n",
    "            delta = np.dot(delta, self.w[i + 1].T) * self.activations[i + 1].gradient(z[i + 1])\n",
    "            dw = np.dot(a[i].T, delta)\n",
    "            update_params[i] = (dw, delta)\n",
    "\n",
    "        # Update the parameters\n",
    "        for k, (dw, delta) in update_params.items():\n",
    "            self.optimizer.step(weights=self.w[k], gradients=dw)\n",
    "            self.optimizer.step(weights=self.b[k], gradients=np.mean(delta, axis=0))\n",
    "\n",
    "    def fit(self, X, y, epochs, batch_size, print_every=np.inf):\n",
    "        \"\"\"Trains the neural network.\n",
    "\n",
    "        Parameters:\n",
    "            X (array of shape (n_samples, n_features))\n",
    "            y (array of shape (n_samples, n_targets))\n",
    "            epochs (int)\n",
    "            batch_size (int)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # As a convention we expect y to be 2D, even if there is only one target to predict\n",
    "        if y.ndim == 1:\n",
    "            y = np.expand_dims(y, axis=1)\n",
    "\n",
    "        # Go through the epochs\n",
    "        for i in range(epochs):\n",
    "\n",
    "            # Shuffle the data\n",
    "            idx = np.arange(X.shape[0])\n",
    "            np.random.shuffle(idx)\n",
    "            x_ = X[idx]\n",
    "            y_ = y[idx]\n",
    "\n",
    "            # Iterate over the training data in mini-batches\n",
    "            for j in range(X.shape[0] // batch_size):\n",
    "                start = j * batch_size\n",
    "                stop = (j + 1) * batch_size\n",
    "                z, a = self._feed_forward(x_[start:stop])\n",
    "                self._backprop(z, a, y_[start:stop])\n",
    "\n",
    "            # Display the performance every print_every eooch\n",
    "            if (i + 1) % print_every == 0:\n",
    "                y_pred = self.predict(X)\n",
    "                print(f'[{i+1}] train loss: {self.loss.loss(y, y_pred)}')\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predicts an output for each sample in X.\n",
    "\n",
    "        Parameters:\n",
    "            X (array of shape (n_samples, n_features))\n",
    "\n",
    "        \"\"\"\n",
    "        _, a = self._feed_forward(X)\n",
    "        return a[self.n_layers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "California housing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input normalization\n",
    "\n",
    "When you're doing gradient descent, the scale of the features matters a lot. Indeed the magnitudes of the gradient descent steps are influenced by the absolute values of the features. If the features are too large, then the gradient steps might be too large and the model will diverge.\n",
    "\n",
    "**Always scale your data**\n",
    "\n",
    "99% of the time, it is recommended to scale your data so that each feature has mean 0 and standard deviation 1. See [this](http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html) for a deeper explanation.\n",
    " \n",
    "You can use scikit-learn's `scale` method from the `preprocessing` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10] train loss: 11.796707532482444\n",
      "[20] train loss: 9.700941500985953\n",
      "[30] train loss: 9.023612069639709\n",
      "2.505495393489851\n"
     ]
    }
   ],
   "source": [
    "housing = fetch_california_housing()\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X = housing[\"data\"]\n",
    "y = housing[\"target\"]\n",
    "\n",
    "X = preprocessing.scale(X)\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y,\n",
    "    test_size=.3,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "nn = NN(\n",
    "    dimensions=(8, 10, 1),\n",
    "    activations=(ReLU, Identity),\n",
    "    loss=MSE,\n",
    "    optimizer=SGD(learning_rate=1e-3)\n",
    ")\n",
    "nn.fit(X_train, y_train, epochs=30, batch_size=8, print_every=10)\n",
    "\n",
    "y_pred = nn.predict(X_test)\n",
    "\n",
    "print(metrics.mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10] train loss: 0.008308476136280957\n",
      "[20] train loss: 0.004984925198988307\n",
      "[30] train loss: 0.004102445263740696\n",
      "[40] train loss: 0.0029634369443098745\n",
      "[50] train loss: 0.0018708680417568045\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        53\n",
      "           1       0.96      0.98      0.97        50\n",
      "           2       0.94      1.00      0.97        47\n",
      "           3       0.96      0.96      0.96        54\n",
      "           4       0.98      1.00      0.99        60\n",
      "           5       0.94      0.97      0.96        66\n",
      "           6       0.98      0.98      0.98        53\n",
      "           7       1.00      0.98      0.99        55\n",
      "           8       1.00      0.93      0.96        43\n",
      "           9       0.98      0.93      0.96        59\n",
      "\n",
      "    accuracy                           0.97       540\n",
      "   macro avg       0.98      0.97      0.97       540\n",
      "weighted avg       0.97      0.97      0.97       540\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "X, y = datasets.load_digits(return_X_y=True)\n",
    "\n",
    "# One-hot encode y\n",
    "y = np.eye(10)[y]\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y,\n",
    "    test_size=.3,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "nn = NN(\n",
    "    dimensions=(64, 15, 10),\n",
    "    activations=(ReLU, Sigmoid),\n",
    "    loss=MSE,\n",
    "    optimizer=SGD(learning_rate=1e-3)\n",
    ")\n",
    "nn.fit(X_train, y_train, epochs=50, batch_size=16, print_every=10)\n",
    "\n",
    "y_pred = nn.predict(X_test)\n",
    "\n",
    "print(metrics.classification_report(y_test.argmax(1), y_pred.argmax(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you want to understand what's going on under the hood of your favorite deep learning framework [here](https://github.com/3outeille/Yaae)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Deep learning framework : Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch is a deep learning framework allowing to automate many operations.\n",
    "It also provides a lot of tools to create and train deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deep learning, the type of data used is the **Tensor**.\n",
    "A tensor is an array of data that can contain vectors,images and much more ! \n",
    "\n",
    "\n",
    "You can instantiate a tensor using the `torch.tensor` method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.0: </b>\n",
    "* Create a tensor `age_1` containing your age with type `long` and a tensor `size_1` containing your height in cm with type `float32`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_1 = ...\n",
    "size = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.1: </b>  \n",
    "Imagine your task is to predict the life expectancy of a person $y_i$ from a set of bioligical measure $(x_i^0,x_i^1,x_i^2,\\ldots,x_i^6)$.\n",
    "* Create three tensors `x_1`, `x_2` and `x_3` of biological data and form a data batch of size 3 `batch_x` with these three tensors.\n",
    "\n",
    "**Tip**: Use `torch.stack`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = ...\n",
    "x2 = ...\n",
    "x3 = ...\n",
    "batch_x = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also check some attributes of your tensor.\n",
    "For example you can look at the shape of the tensor using the `shape` attribute, the gradient of a tensor using the `grad` attribute and the type using `dtype`.\n",
    "\n",
    "You can also see under which device your tensor is with the attribute `device`.\n",
    "Finally you can also put your data on gpu using the `torch.tensor.to` method.\n",
    "The possible devices are \"cpu\" and \"cuda\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.2: </b>  \n",
    "* Look at the device, gradient, type and shape of your tensors.\n",
    "* Change the device of your tensors to \"cuda\".\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now you will using again the California housing dataset.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To manage more easily the processing of your data, pytorch proposes a tool: the `Dataset` class.\n",
    "\n",
    "This class allows the creation of a data generator which will be very useful when training your model.\n",
    "\n",
    "This dataset allows to retrieve the data as a tensor.\n",
    "The dataset implements 2 methods : The `__get_item__` method which allows to access to a sample and the `__len__` method which returns the number of sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.4: </b>  \n",
    "Implement a torch dataset allowing you to access the boston dataset.\n",
    "* The `__get_item__` method should return a dict containing the data and labels in tensor form.\n",
    "* The `__len__` method should return the number of samples.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularDataset(Dataset):\n",
    "  def __init__(self,X,y):\n",
    "    self.X = ...\n",
    "    self.y = ...\n",
    "  def __getitem__(self,idx): \n",
    "    return {\"data\": ...,\"label\": ...}\n",
    "\n",
    "  def __len__(self):\n",
    "    return ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.5: </b>  \n",
    "Set up a dataset for the train set and one for the validation set\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ...\n",
    "val_dataset = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"First sample (x,y) of the dataset : {train_dataset[0]} \\n\") # get_item method\n",
    "print(f\"There are {len(train_dataset)} samples in the dataset.\") # len method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dataset is created, another class is used to wrap this dataset: The `Dataloader`.\n",
    "A dataloader allows to sample batches of dataset data and to parallelize the batch formation on several workers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batching\n",
    "\n",
    "At each iteration $i$, instead of updating the weights $w_i$ by using the gradient $g_i$, we can accumulate the gradients and only update the weights every $k$ iterations.\n",
    "\n",
    "The gradient we will use to update the weights will thus be the average of the past $k$ gradients. This is called **mini-batch gradient descent**. Stochastic gradient descent can be seen as a special case of mini-batch gradient descent when the batch size is set to 1.\n",
    "\n",
    "![mini-batch](mini-batch.png)\n",
    "\n",
    "The batch size is important. Small batch size work well because they are a form of regula\n",
    "\n",
    "Here are some links if you want to get some intuitions:\n",
    "\n",
    "- [Tradeoff batch size vs. number of iterations to train a neural network - Cross Validated](https://stats.stackexchange.com/questions/164876/tradeoff-batch-size-vs-number-of-iterations-to-train-a-neural-network)\n",
    "- [What is batch size in neural network? - Cross Validated](https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network)\n",
    "- [In deep learning, why don't we use the whole training set to compute the gradient? - Quora](https://www.quora.com/In-deep-learning-why-dont-we-use-the-whole-training-set-to-compute-the-gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.6: </b>  \n",
    "Create a train dataloader and a validation dataloader that returns sample batches of size 16.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = ...\n",
    "num_workers = ...\n",
    "train_dataloader = ...\n",
    "val_dataloader = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.7: </b>  \n",
    "* Inspect the first batch of your training loader.\n",
    "* Create a data variable and a label variable containing respectively, the data and the labels of the first batch\n",
    "\n",
    "**Tip**: To get the first element of the loader use: `next(iter(loader))`\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch = ...\n",
    "data = ...\n",
    "label = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class MLPRegression(nn.Module): # All pytorch models must inherit the nn.Module\n",
    "  def __init__(self,in_features):\n",
    "     super(MLPRegression, self).__init__() # The constructor of the class calls the constructor of its parent class with the keyword \"super\".\n",
    "     self.layer1 = ...\n",
    "  def forward(self,x):\n",
    "    value = ...\n",
    "    return value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.9: </b>  \n",
    "Use your neural network to make a prediction on the first batch of data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = ...\n",
    "mlp_regression = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will move on to the most important part of the session: Training the model.\n",
    "Writing a **training loop** is not a simple thing at first.\n",
    "The following code implements a generic training loop that I advise you to keep for your future work in deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.10: </b>  \n",
    "* Explain the role of each argument of the function\n",
    "* Fill in the blank code\n",
    "* Comment on each line of the training loop (except the lines used for visualization).\n",
    "* Raise your hand to give me an oral report on this question \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "sns.set()\n",
    "def train_regressor(\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    train_loader: DataLoader,\n",
    "    valid_loader: DataLoader,\n",
    "    nb_epoch: int,\n",
    "    criterion: nn.Module,\n",
    "    batch_size:int=16,\n",
    "    device: torch.device = torch.device(\"cuda:0\"),\n",
    "    \n",
    "    verbose: bool=True,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Pytorch training loop\n",
    "    Args:\n",
    "        model (nn.Module): Pytorch classification model\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for the model\n",
    "        train_loader (DataLoader): DataLoader for training fold\n",
    "        valid_loader (DataLoader): DataLoader for validation fold\n",
    "        nb_epoch (int): Number of epoch\n",
    "        criterion (nn.Module): Loss\n",
    "        device (torch.device): .Defaults to `torch.device(\"cuda:0\")`\n",
    "        verbose (bool): Verbose term\n",
    "    \"\"\"\n",
    "    loaders = {\"train\": train_loader, \"validation\": valid_loader}\n",
    "    model.to(device)\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for epoch in range(1, nb_epoch + 1):\n",
    "        if verbose:\n",
    "          print(\"-\" * 80)\n",
    "        for phase in [\"train\", \"validation\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            running_loss = 0.0\n",
    "            for sample in loaders[phase]:\n",
    "                data = ...\n",
    "                label = ...\n",
    "                optimizer.zero_grad()\n",
    "                data, label = data.to(device), label.to(device)\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    output = ...\n",
    "                    pred_label = ...\n",
    "                    loss = ...\n",
    "                    if phase == \"train\":\n",
    "                        ... # Compute the gradient\n",
    "                        ... # Make a gradient step\n",
    "                running_loss += loss.item()\n",
    "            epoch_loss = running_loss / (len(loaders[phase].dataset)/batch_size)\n",
    "            if phase == \"train\":\n",
    "              train_loss.append(epoch_loss)\n",
    "            else:\n",
    "              val_loss.append(epoch_loss)\n",
    "            if verbose:\n",
    "              print(\n",
    "                  f\" Epoch number: {epoch}, Phase: {phase}, Loss value: {epoch_loss:.4f}\"\n",
    "              )\n",
    "    fig,(axe1,axe2) = plt.subplots(2,figsize=(10,10))\n",
    "    fig.suptitle('Training and validation statistics')\n",
    "\n",
    "\n",
    "    y_pred = model(torch.from_numpy(X_val).to(device)).detach().cpu().numpy()\n",
    "    axe1.plot(np.arange(len(train_loss)),train_loss,label=\"train MSE\")\n",
    "    axe1.plot(np.arange(len(val_loss)),val_loss,label = \"val MSE\")\n",
    "    axe1.set_title(\"MSE loss\")\n",
    "    axe1.legend()\n",
    "    axe2.scatter(range(len(y_val)), scaler.inverse_transform(y_val), label='target');\n",
    "    axe2.scatter(range(len(y_val)), scaler.inverse_transform(y_pred), label='prediction');\n",
    "    axe2.set_title(f\"Prediction // MSE = {mean_absolute_error(y_pred, y_val)}\")\n",
    "    axe1.legend()\n",
    "    plt.show()\n",
    "    return train_loss,val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.11: </b>  \n",
    "Instantiate the list of parameters necessary to launch the function and justify each of these parameters (Why use this loss, why use this optimizer, why set this learning rate, why use this architecture ...)  \n",
    "</div>\n",
    "\n",
    "**Tip**: Use `Adam` optimizer and have a look to https://pytorch.org/docs/stable/nn.html#loss-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam,SGD\n",
    "mlp_logistic = ...\n",
    "batch_size = ...\n",
    "num_workers = ...\n",
    "train_dataloader = ...\n",
    "val_dataloader = ...\n",
    "learning_rate = ...\n",
    "optimizer = ...\n",
    "criterion = ...\n",
    "device = ...\n",
    "nb_epochs = ...\n",
    "verbose = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss,val_loss= ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.12: </b>  \n",
    "Comment the results on : \n",
    "* The loss function\n",
    "* The model error\n",
    "\n",
    "Can we do better and how?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 3.13: </b>  \n",
    "* Implement a new deeper architecture.\n",
    "* Set the necessary parameters for the training again and restart the procedure.\n",
    "* Comment on the new results obtained\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class MLPDeep(nn.Module):\n",
    "  def __init__(self,in_features):\n",
    "     super(MLPDeep, self).__init__()\n",
    "     ...\n",
    "  def forward(self,x):\n",
    "    ...\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_deep = ...\n",
    "batch_size = ...\n",
    "num_workers = ...\n",
    "train_dataloader = ...\n",
    "val_dataloader = ...\n",
    "learning_rate = ...\n",
    "optimizer = ...\n",
    "criterion = ...\n",
    "device = ...\n",
    "nb_epochs = ...\n",
    "verbose = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss,val_loss= ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.14: </b>  \n",
    "* Train the model for 100 epochs and comment on the results obtained.\n",
    "* Plot the loss function and the model error as a function of the epochs. For the training and validation set.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The phenomena of exercise 1.14 is called overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Exercise 1.15: </b>   \n",
    "* Explain in your own words what is overfitting.\n",
    "* How can we avoid this phenomenon?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 3.14: </b>  \n",
    "* Implement early stopping in the training loop.\n",
    "* Add some regularization \n",
    "* Add weight decay on the optimizer and explain in your own words what is weight decay.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "In the previous cells we just mentionned the word **regularization**, what is it exactly?\n",
    "\n",
    "1. A machine learning model learns from a training set \n",
    "2. We want the model to perform well on a test set it hasn't seen\n",
    "3. The stronger the model, the more there is a chance that it overfits by memorizing patterns that only exists in the training set\n",
    "4. Regularizing a model means that we make it's life harder  \n",
    "5. There are many ways to regularize a neural network:\n",
    "    1. Use more data! The more training data there is, the more the model will focus on general patterns\n",
    "    2. Penalize the updates made to the weights by the optimizer \n",
    "    3. Use dropout\n",
    "    4. Use batch normalization\n",
    "    5. Use early stopping\n",
    "    \n",
    "![complexity](complexity.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How di I choose the architecture of my neural network ?\n",
    "There are so many choices you can make that it can quickly become overwhelming. Choosing the right pieces of the puzzle is very much an art rather than a science. There is no getting around trying things out. It is thus very important to setup a stable environment for testing and evaluating model choices. You should always start by defining a reliable testing procedure.\n",
    "\n",
    "Here is some general advice:\n",
    "\n",
    "1. Start with a very simple model (for example a logistic regression)\n",
    "2. Add complexity to the model as long as the validation score improves\n",
    "3. If the model is overfitting (you can detect it by comparing the training and validation scores) then add regularization\n",
    "4. Last but not least, spend most of your time checking that the data you're using is correct\n",
    "\n",
    "Here are some more links if you're interested:\n",
    "\n",
    "- [How to decide neural network architecture? - Data Science exchange](https://datascience.stackexchange.com/questions/20222/how-to-decide-neural-network-architecture)\n",
    "- [How to choose the number of hidden layers and nodes in a feedforward neural network? - Cross Validated](https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
