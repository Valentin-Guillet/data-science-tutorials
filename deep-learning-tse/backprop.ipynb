{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Deep Learning?\n",
    "\n",
    "**Deep Learning** is a specialized sub-field of **Machine Learning (ML)**, which is itself a core discipline within **Artificial Intelligence (AI)**.\n",
    "* **Artificial Intelligence (AI):** aims to enable computers to **automatically learn concepts** and solve problems that typically require human intelligence.\n",
    "* **Machine Learning (ML):** focuses on enabling systems to learn these concepts **from data/examples** without being explicitly programmed for a specific task.\n",
    "* **Deep Learning (DL):** a specific, powerful category of Machine Learning methods characterized by the use of **deep neural networks** (i.e., neural networks with multiple hidden layers) to model complex patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Learning Tasks in Machine Learning\n",
    "\n",
    "In **Machine Learning (ML)**, the goal is to train an algorithm to perform a specific task by exposing it to a dataset of examples. Crucially, the algorithm must learn **generalizable patterns** from the data rather than simply memorizing the training examples.\n",
    "\n",
    "We typically represent a single input example as a vector $\\mathbf{x}$. In **supervised learning**, this input is paired with a desired output, or **label**, $\\mathbf{y}$.\n",
    "\n",
    "Below are several common types of learning tasks, categorized by the nature of their output and goal:\n",
    "\n",
    "* **Classification**: The algorithm's objective is to predict a discrete label $\\mathbf{y}$ from a set of $k$ possible classes.\n",
    "    * **Binary Classification**: A specific case where $k=2$ (e.g., Yes/No, Spam/Not Spam).\n",
    "    * **Multi-Output Classification**: The algorithm predicts multiple labels or tags for a single input example (e.g., classifying all objects present in an image).<br><br>\n",
    "\n",
    "* **Regression**: The algorithm's objective is to predict one or multiple continuous, real-valued output $\\mathbf{y}$ (e.g., predicting house prices or temperature).\n",
    "\n",
    "* **Denoising**: The algorithm takes a corrupted or noisy input vector, $\\tilde{\\mathbf{x}}$, and is trained to produce the corresponding clean output vector, $\\mathbf{x}$.\n",
    "\n",
    "* **Auto-encoding (Representation Learning)**: The algorithm is trained to reconstruct its input $\\mathbf{x}$ as its output $\\hat{\\mathbf{x}}$. The network is structurally constrained (usually by a bottleneck layer) to learn an efficient, compressed representation of the input. Failure to reproduce a specific input accurately can indicate that the input is an **outlier or anomaly** with respect to the training data distribution.\n",
    "\n",
    "* **Density Estimation**: The algorithm aims to learn and output the **probability density function** $p(\\mathbf{x})$ of the input vector $\\mathbf{x}$. Essentially, the learning algorithm models the underlying, non-parametric probability distribution of the data.\n",
    "\n",
    "* **Optimal Control / Reinforcement Learning (RL)**: The algorithm (known as an **Agent**) inputs a state $\\mathbf{s}$ of a **dynamic system or environment** and must output an optimal action $\\mathbf{a}$. This action is then applied to the environment, leading to a new state and a reward signal.\n",
    "\n",
    "* **Generative Models**: The goal is to generate new data samples $\\mathbf{x}_{\\text{gen}}$ that belong to the same distribution $p(\\mathbf{x})$ as the original training data. Examples include **Generative Adversarial Networks (GANs)** and **Diffusion Models**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks: Core Architecture and Function\n",
    "\n",
    "A **Neural Network** is fundamentally a sequence of connected computational units organized into **layers**. Each layer performs a transformation on its input to produce an output, which then serves as the input for the next layer.\n",
    "\n",
    "We can denote the transformation performed by a single layer as a function $f$:\n",
    "$$ \\mathbf{z} = f(\\mathbf{x}) $$\n",
    "\n",
    "Here, $\\mathbf{x}$ is the input (either the initial feature vector or the output from a preceding layer), and $\\mathbf{z}$ is the layer's output.\n",
    "\n",
    "In the common case of a **Dense** (or **Fully Connected**) layer, the function $f$ is an **affine transformation**:\n",
    "$$ \\mathbf{z} = \\mathbf{W}\\mathbf{x} + \\mathbf{b} $$\n",
    "\n",
    "Where $\\mathbf{W}$ is the **weight matrix** and $\\mathbf{b}$ is the **bias vector**. These are the **trainable parameters** of the layer.\n",
    "\n",
    "When layers are stacked, we are simply chaining these functions:\n",
    "$$ \\hat{\\mathbf{y}} = f_N(f_{N-1}(\\dots(f_1(\\mathbf{x})))) $$\n",
    "\n",
    "If all layers are purely linear (i.e., Dense layers without modification), chaining them together ultimately results in a single, large linear transformation. This means a purely linear network, no matter how deep, cannot learn complex, **non-linear decision boundaries** (like the XOR function).\n",
    "\n",
    "To overcome this limitation and enable the network to model complex data, we introduce a **non-linear activation function** $g$ after the linear transformation of each layer.\n",
    "\n",
    "The output from a layer followed by its activation function, $\\mathbf{a}$, is:\n",
    "$$ \\mathbf{a} = g(\\mathbf{z}) = g(\\mathbf{W}\\mathbf{x} + \\mathbf{b}) $$\n",
    "\n",
    "When layers are stacked with non-linearity, the final network output is:\n",
    "$$ \\hat{\\mathbf{y}} = g_N(\\mathbf{W}_N g_{N-1}(\\mathbf{W}_{N-1} \\dots g_1(\\mathbf{W}_1\\mathbf{x} + \\mathbf{b}_1) \\dots) + \\mathbf{b}_N) $$\n",
    "\n",
    "While there are many specialized network architectures (Convolutional, Recurrent, etc.), the general principle remains: we seek to **tune the weights ($\\mathbf{W}$) and biases ($\\mathbf{b}$)** of every layer. This optimization process, typically performed using an algorithm called **Backpropagation**, is what enables the network to learn the desired mapping from $\\mathbf{x}$ to $\\hat{\\mathbf{y}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "\n",
    "A **Loss Function** (or **Objective Function**), usually denoted $L$, quantifies the discrepancy between the network's prediction $\\hat{\\mathbf{y}}$ and the true target value $\\mathbf{y}$. It takes these two vectors as input and outputs a single **scalar value**.\n",
    "$$ L(\\mathbf{y}, \\hat{\\mathbf{y}}) \\in \\mathbb{R} $$\n",
    "\n",
    "The fundamental goal of the learning algorithm is to **minimize** this loss function across the training dataset, effectively teaching the network to make predictions that are as close to the ground truth as possible.\n",
    "\n",
    "Here are several widely used loss functions:\n",
    "\n",
    "#### 1. Mean Squared Error (MSE) / $\\ell_2$ Loss\n",
    "$$ L(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n",
    "* **Primary Use:** **Regression** tasks where the output $\\hat{y}$ is a continuous value.\n",
    "* **Characteristics:** Penalizes large errors significantly more than small errors due to the squaring operation.<br><br>\n",
    "\n",
    "#### 2. Cross-Entropy Loss (Log Loss)\n",
    "For true label $y \\in \\{0, 1\\}$ and predicted probability $\\hat{p}$:\n",
    "    $$ L(y, \\hat{p}) = - [y \\log(\\hat{p}) + (1-y) \\log(1-\\hat{p})] $$\n",
    "* **Primary Use:** The standard loss for **Classification** tasks, especially when estimating class probabilities.\n",
    "* **Characteristics:** Highly penalizes confident but incorrect predictions. It measures the dissimilarity between the true probability distribution ($\\mathbf{y}$) and the predicted distribution ($\\hat{\\mathbf{p}}$).<br><br>\n",
    "\n",
    "#### 3. Poisson Loss\n",
    "$$ L(y, \\hat{y}) = \\hat{y} - y \\log(\\hat{y}) $$\n",
    "* **Primary Use:** Specialized **Regression** tasks where the target variable $\\mathbf{y}$ represents **counts** (e.g., number of events, call arrivals). It assumes the underlying data follows a Poisson distribution.<br><br>\n",
    "\n",
    "\n",
    "#### 4. Hinge Loss\n",
    "$$ L(y, p) = \\max(0, 1 - y p) $$\n",
    "* **Primary Use:** Primarily used for training **Support Vector Machines (SVMs)**, but can be adapted for neural networks in classification tasks.\n",
    "* **Characteristics:** Aims for a margin of separation. It only incurs a penalty if the prediction $p$ is on the wrong side of the margin (where $y \\in \\{-1, 1\\}$).\n",
    "\n",
    "### The Impact of Loss\n",
    "\n",
    "The choice of loss function is critical as it dictates the network's learning priorities. For example, **Linear Regression** is a linear model optimized using the **Squared Loss**, while **Logistic Regression** is a linear model optimized using the **Cross-Entropy/Logistic Loss**. The model architecture remains simple, but the change in the optimization objective completely transforms the type of problem solved (regression vs. classification/probability estimation).\n",
    "\n",
    "The Machine Learning community continually develops specialized loss functions, such as the **Focal Loss** for handling highly imbalanced datasets or those designed to support **importance weighting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Backpropagation Algorithm\n",
    "\n",
    "The **Backpropagation** algorithm (short for \"backward propagation of errors\") is the fundamental method used to efficiently calculate the gradient of the loss function with respect to every weight and bias in a neural network. It relies entirely on the **Chain Rule** from calculus.\n",
    "\n",
    "The chain rule allows us to calculate the derivative of a composite function. If we have a function $h$ defined as the composition of two functions, $g$ and $f$, applied to an input $\\mathbf{x}$, such that $h(\\mathbf{x}) = g(f(\\mathbf{x}))$, then the derivative of $h$ with respect to $\\mathbf{x}$ is:\n",
    "\n",
    "$$\\frac{\\partial h}{\\partial \\mathbf{x}} = \\frac{\\partial g}{\\partial f} \\times \\frac{\\partial f}{\\partial \\mathbf{x}}$$\n",
    "\n",
    "This principle is directly applicable to neural networks, which are nothing more than a long chain of composed functions (layers). To find out how a parameter deep within the network affects the final loss, we simply apply the chain rule across all intermediate layers.\n",
    "\n",
    "To train the network, we must calculate the gradient of the **Loss Function** $L$ with respect to the network's trainable parameters, $\\mathbf{W}$ (weights) and $\\mathbf{b}$ (biases). Let's start with the parameters of the final layer, $N$: $\\mathbf{W}^{(N)}$ and $\\mathbf{b}^{(N)}$.\n",
    "\n",
    "A single layer's computation involves three consecutive steps:\n",
    "\n",
    "1.  **Linear Transformation:** $\\mathbf{z}^{(N)} = \\mathbf{W}^{(N)}\\mathbf{a}^{(N-1)} + \\mathbf{b}^{(N)}$ with $\\mathbf{a}^{(N-1)}$ the output of the previous layer (after activation)\n",
    "2.  **Activation:** $\\mathbf{a}^{(N)} = g(\\mathbf{z}^{(N)})$\n",
    "3.  **Loss Calculation:** $L(\\mathbf{y}, \\mathbf{a}^{(N)})$\n",
    "\n",
    "To find the gradient $\\frac{\\partial L}{\\partial \\mathbf{W}^{(N)}}$, we apply the chain rule across these three steps:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{W}^{(N)}} = \\frac{\\partial L}{\\partial \\mathbf{a}^{(N)}} \\times \\frac{\\partial \\mathbf{a}^{(N)}}{\\partial \\mathbf{z}^{(N)}} \\times \\frac{\\partial \\mathbf{z}^{(N)}}{\\partial \\mathbf{W}^{(N)}}$$\n",
    "\n",
    "Similarly, the gradient calculation for the biases $\\mathbf{b}$ follows the same chain rule structure. Since $\\mathbf{z} = \\mathbf{W}\\mathbf{x} + \\mathbf{b}$, the derivative of the linear output $\\mathbf{z}$ with respect to the bias $\\mathbf{b}$ is simply the identity matrix (or 1 in the scalar case). For the final layer bias $\\mathbf{b}^{(N)}$:\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{b}^{(N)}} = \\frac{\\partial L}{\\partial \\mathbf{a}^{(N)}} \\times \\frac{\\partial \\mathbf{a}^{(N)}}{\\partial \\mathbf{z}^{(N)}} \\times \\frac{\\partial \\mathbf{z}^{(N)}}{\\partial \\mathbf{b}^{(N)}} = \\frac{\\partial L}{\\partial \\mathbf{a}^{(N)}} \\times \\frac{\\partial \\mathbf{a}^{(N)}}{\\partial \\mathbf{z}^{(N)}} \\times 1$$\n",
    "\n",
    "#### Example: MSE Loss and Sigmoid Activation\n",
    "\n",
    "Let's use the specific example of a single output unit with **Mean Squared Error (MSE) Loss** and a **Sigmoid Activation** function $g(\\mathbf{z}) = \\sigma(\\mathbf{z}) = \\frac{1}{1 + e^{-z}}$.\n",
    "\n",
    "1.  Gradient of Loss w.r.t. activation output:\n",
    "    $$\\frac{\\partial L}{\\partial \\mathbf{a}^{(N)}} = \\frac{\\partial (\\mathbf{a}^{(N)} - y)^2}{\\partial \\mathbf{a}^{(N)}} = 2 \\times (\\mathbf{a}^{(N)} - y)$$\n",
    "2.  Gradient of Activation w.r.t. linear output: (reminder: $\\sigma'(x) = \\sigma(x) (1 - \\sigma(x))$)\n",
    "    $$\\frac{\\partial \\mathbf{a}^{(N)}}{\\partial \\mathbf{z}^{(N)}} = \\frac{\\partial \\sigma(\\mathbf{z}^{(N)})}{\\partial \\mathbf{z}^{(N)}} = \\sigma(\\mathbf{z}^{(N)}) (1 - \\sigma(\\mathbf{z}^{(N)}))$$\n",
    "3.  Gradient of Linear Output w.r.t. weights: since $\\mathbf{z} = \\mathbf{W}\\mathbf{x} + \\mathbf{b}$, the derivative w.r.t. $\\mathbf{W}$ is the input $\\mathbf{x}$. In this case, the input to the last layer is $\\mathbf{a}^{(N-1)}$:\n",
    "    $$\\frac{\\partial \\mathbf{z}^{(N)}}{\\partial \\mathbf{W}^{(N)}} = \\frac{\\partial \\left(\\mathbf{W}^{(N)}\\mathbf{a}^{(N-1)} + \\mathbf{b}^{(N)}\\right)}{\\partial \\mathbf{W}^{(N)}} = \\mathbf{a}^{(N-1)}$$\n",
    "\n",
    "Combining these terms gives the full gradient for the last layer weights:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{W}^{(N)}} = 2 (\\mathbf{a}^{(N)} - y) \\times \\sigma(\\mathbf{z}^{(N)}) (1 - \\sigma(\\mathbf{z}^{(N)})) \\times \\mathbf{a}^{(N-1)}$$\n",
    "\n",
    "Similarly for the biais gradient:\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{b}^{(N)}} = 2 (\\mathbf{a}^{(N)} - y) \\times \\sigma(\\mathbf{z}^{(N)}) (1 - \\sigma(\\mathbf{z}^{(N)})) \\times 1$$\n",
    "\n",
    "### The Efficiency of Backward Propagation\n",
    "\n",
    "Now consider the parameters of the layer $N-1$. The full chain rule for the gradient $\\frac{\\partial L}{\\partial \\mathbf{W}^{(N-1)}}$ is:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}^{(N-1)}} &= \\frac{\\partial L}{\\partial \\mathbf{a}^{(N-1)}} \\times \\frac{\\partial \\mathbf{a}^{(N-1)}}{\\partial \\mathbf{z}^{(N-1)}} \\times \\frac{\\partial \\mathbf{z}^{(N-1)}}{\\partial \\mathbf{W}^{(N-1)}} \\\\\n",
    "&= \\left( \\frac{\\partial L}{\\partial \\mathbf{a}^{(N)}} \\times \\frac{\\partial \\mathbf{a}^{(N)}}{\\partial \\mathbf{z}^{(N)}} \\times \\frac{\\partial \\mathbf{z}^{(N)}}{\\partial \\mathbf{a}^{(N-1)}} \\right) \\times \\frac{\\partial \\mathbf{a}^{(N-1)}}{\\partial \\mathbf{z}^{(N-1)}} \\times \\frac{\\partial \\mathbf{z}^{(N-1)}}{\\partial \\mathbf{W}^{(N-1)}}\n",
    "\\end{align}\n",
    "\n",
    "The term in parentheses represents how the loss changes with respect to the output of the *previous* layer, $\\mathbf{a}^{(N-1)}$.\n",
    "\n",
    "**The Key Insight of Backpropagation:** When computing the gradient for layer $N-1$, we **reuse** the product of the derivatives already computed for the gradient of layer $N$. We simply need to calculate the *local* derivatives for layer $N-1$ and multiply them by the \"error signal\" $\\frac{\\partial L}{\\partial \\mathbf{a}^{(N-1)}}$ passed backward from layer $N$. This recursive and efficient structure is why the algorithm is named **Backpropagation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (SGD)\n",
    "\n",
    "**Stochastic Gradient Descent (SGD)** is the most fundamental optimization algorithm used to train neural networks. Its core purpose is to iteratively adjust the network's parameters (weights and biases) to minimize the overall loss function.\n",
    "\n",
    "SGD differs from standard (Batch) Gradient Descent by computing the gradient and updating parameters based on a small subset of the data:\n",
    "\n",
    "1.  **Gradient Calculation** (The \"Stochastic\" Step): For a randomly selected single data point or a small batch of observations $(\\mathbf{x}_i, \\mathbf{y}_i)$, the gradient $\\nabla L(\\mathbf{W})$ is computed. This gradient $\\nabla_i$ represents an **approximation** of the true gradient over the entire dataset.\n",
    "2.  **Parameter Update** (The \"Descent\" Step): An **optimizer** updates the current weights $\\mathbf{W}_i$ to obtain the new weights $\\mathbf{W}_{i+1}$ by moving in the direction opposite to the gradient.\n",
    "3.  **Epochs:** A complete pass through the entire training dataset is called an **epoch**. Training typically involves multiple epochs to ensure convergence.\n",
    "\n",
    "The general formulation for the weight update in SGD is:\n",
    "$$\\mathbf{W}_{i+1} \\leftarrow \\mathbf{W}_i - \\alpha_i \\nabla_i$$\n",
    "\n",
    "where:\n",
    "* $\\mathbf{W}_i$ are the current parameters (weights and biases).\n",
    "* $\\nabla_i$ is the calculated gradient of the loss function $L$ with respect to the parameters $\\mathbf{W}_i$.\n",
    "* $\\alpha_i$ is the **learning rate** (or step size) at iteration $i$.\n",
    "\n",
    "The **learning rate ($\\alpha_i$)** is a critical hyperparameter that determines the size of the steps taken toward the minimum. Choosing an appropriate learning rate is crucial for effective training, a topic we will address shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 0: </b>  \n",
    "In your words comment the following code. Explain what is happening at each step.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets, metrics, model_selection, preprocessing\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "class ReLU:\n",
    "    \"\"\"Rectified Linear Unit (ReLU) activation function.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def activation(z: np.ndarray) -> np.ndarray:\n",
    "        z = np.maximum(0, z)\n",
    "        return z\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient(z: np.ndarray) -> np.ndarray:\n",
    "        z[z < 0] = 0\n",
    "        z[z > 0] = 1\n",
    "        return z\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    \"\"\"Sigmoid ($\\sigma$) activation function.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def activation(z: np.ndarray) -> np.ndarray:\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient(z: np.ndarray) -> np.ndarray:\n",
    "        s = Sigmoid.activation(z)\n",
    "        return s * (1 - s)\n",
    "\n",
    "\n",
    "class Identity:\n",
    "    \"\"\"Identity activation function.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def activation(z: np.ndarray) -> np.ndarray:\n",
    "        return z\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient(z: np.ndarray) -> np.ndarray:\n",
    "        return np.ones_like(z)\n",
    "\n",
    "\n",
    "class MSE:\n",
    "    \"\"\"Mean Squared Error (MSE) loss function.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def loss(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        return np.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient(y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "        return 2 * (y_pred - y_true)\n",
    "\n",
    "\n",
    "class SGD:\n",
    "    \"\"\"Stochastic Gradient Descent (SGD) optimizer.\"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate: float):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def step(self, weights: np.ndarray, gradients: np.ndarray) -> None:\n",
    "        weights -= self.learning_rate * gradients\n",
    "\n",
    "\n",
    "class NN:\n",
    "    \"\"\"A simple fully-connected Neural Network model.\"\"\"\n",
    "\n",
    "    def __init__(self, dimensions: list[int], activations: list[Any], loss, optimizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dimensions: List of layer sizes, e.g., [input_size, hidden_1, hidden_2, output_size].\n",
    "            activations: List of activation classes for the hidden and output layers (length: len(dimensions)-1).\n",
    "            loss: The loss function class (e.g., MSE).\n",
    "            optimizer: The optimizer class instance (e.g., SGD).\n",
    "        \"\"\"\n",
    "        self.n_layers = len(dimensions)\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        self.w: dict[int, np.ndarray] = {}  # Weights W[i] connects layer i-1 to i\n",
    "        self.b: dict[int, np.ndarray] = {}  # Biases B[i] for layer i\n",
    "        self.activations: dict[int, Any] = {} # Activation for layer i\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        for i in range(self.n_layers - 1):\n",
    "            input_dim = dimensions[i]\n",
    "            output_dim = dimensions[i + 1]\n",
    "            self.w[i + 1] = np.random.randn(input_dim, output_dim) / np.sqrt(input_dim)\n",
    "            self.b[i + 1] = np.zeros(output_dim)\n",
    "            self.activations[i + 2] = activations[i] # Layer 2 is the first hidden layer\n",
    "\n",
    "    def _feed_forward(self, X: np.ndarray) -> tuple[dict[int, np.ndarray], dict[int, np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Executes a forward pass through the neural network.\n",
    "\n",
    "        Args:\n",
    "            X: Input data batch of shape (batch_size, n_features).\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing:\n",
    "            - z: dict of linear outputs before activation {layer_idx: np.ndarray}.\n",
    "            - a: dict of activation outputs {layer_idx: np.ndarray}. a[1] is the input X.\n",
    "        \"\"\"\n",
    "        z: dict[int, np.ndarray] = {}\n",
    "        a: dict[int, np.ndarray] = {1: X}  # a[1] holds the input X (activation of the input layer)\n",
    "\n",
    "        # Iterate from the first hidden layer (i=2) up to the output layer (n_layers)\n",
    "        for i in range(2, self.n_layers + 1):\n",
    "            # Linear transformation: z = a_prev @ W + b\n",
    "            z[i] = np.dot(a[i - 1], self.w[i - 1]) + self.b[i - 1]\n",
    "            # Non-linear activation: a = g(z)\n",
    "            a[i] = self.activations[i].activation(z[i])\n",
    "\n",
    "        return z, a\n",
    "\n",
    "    def _backprop(self, z: dict[int, np.ndarray], a: dict[int, np.ndarray], y_true: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Performs the backpropagation step to compute and apply gradients.\n",
    "\n",
    "        Args:\n",
    "            z: Linear outputs from forward pass.\n",
    "            a: Activation outputs from forward pass.\n",
    "            y_true: Ground truth target values of shape (batch_size, n_targets).\n",
    "        \"\"\"\n",
    "\n",
    "        # 1. Output Layer (Layer N = self.n_layers)\n",
    "        y_pred = a[self.n_layers]\n",
    "        final_activation = self.activations[self.n_layers]\n",
    "        delta = self.loss.gradient(y_true, y_pred) * final_activation.gradient(z[self.n_layers])\n",
    "        dw = np.dot(a[self.n_layers - 1].T, delta)\n",
    "        update_params = {self.n_layers - 1: (dw, delta)}\n",
    "\n",
    "        # 2. Hidden Layers (Backward pass from L-1 to 1)\n",
    "        # i represents the index of the weight matrix W[i] and bias B[i]\n",
    "        for i in range(self.n_layers - 2, 0, -1):\n",
    "            # Propagate delta backward: delta_i = (delta_{i+1} @ W_{i+1}) * (da_i/dz_i)\n",
    "            delta = np.dot(delta, self.w[i + 1].T) * self.activations[i + 1].gradient(z[i + 1])\n",
    "            # dW_i = a_i @ delta_i\n",
    "            dw = np.dot(a[i].T, delta)\n",
    "            update_params[i] = (dw, delta)\n",
    "\n",
    "        # 3. Parameter Update\n",
    "        for k, (dw, delta) in update_params.items():\n",
    "            # Update weights W[k]\n",
    "            self.optimizer.step(weights=self.w[k], gradients=dw)\n",
    "            # Update biases B[k]: gradient is the mean of delta over the batch\n",
    "            self.optimizer.step(weights=self.b[k], gradients=np.mean(delta, axis=0))\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, epochs: int, batch_size: int, print_every: float = np.inf) -> None:\n",
    "        \"\"\"\n",
    "        Trains the neural network using mini-batch SGD.\n",
    "\n",
    "        Args:\n",
    "            X: Training features of shape (n_samples, n_features).\n",
    "            y: Training targets of shape (n_samples, n_targets).\n",
    "            epochs: Number of complete passes over the training data.\n",
    "            batch_size: Number of samples per gradient update.\n",
    "            print_every: Print training loss every this many epochs.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Convention: y should be 2D\n",
    "        if y.ndim == 1:\n",
    "            y = np.expand_dims(y, axis=1)\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "        n_batches = n_samples // batch_size\n",
    "\n",
    "        for i in range(epochs):\n",
    "            # Shuffle the data indices for random mini-batches\n",
    "            idx = np.arange(n_samples)\n",
    "            np.random.shuffle(idx)\n",
    "            x_shuffled = X[idx]\n",
    "            y_shuffled = y[idx]\n",
    "\n",
    "            # Iterate over mini-batches\n",
    "            for j in range(n_batches):\n",
    "                start = j * batch_size\n",
    "                stop = (j + 1) * batch_size\n",
    "                x_batch = x_shuffled[start:stop]\n",
    "                y_batch = y_shuffled[start:stop]\n",
    "                z, a = self._feed_forward(x_batch)\n",
    "                self._backprop(z, a, y_batch)\n",
    "\n",
    "            # Display performance\n",
    "            if (i + 1) % print_every == 0:\n",
    "                y_pred = self.predict(X)\n",
    "                current_loss = self.loss.loss(y, y_pred)\n",
    "                print(f'[Epoch {i+1}] train loss: {current_loss:.4f}')\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predicts an output for each sample in X.\n",
    "\n",
    "        Args:\n",
    "            X: Input data of shape (n_samples, n_features).\n",
    "\n",
    "        Returns:\n",
    "            Predicted output $\\hat{y}$ of shape (n_samples, n_targets).\n",
    "        \"\"\"\n",
    "        _, a = self._feed_forward(X)\n",
    "        return a[self.n_layers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input normalization\n",
    "\n",
    "When you're doing gradient descent, the scale of the features matters a lot. Indeed the magnitudes of the gradient descent steps are influenced by the absolute values of the features. If the features are too large, then the gradient steps might be too large and the model will diverge.\n",
    "\n",
    "**Always scale your data**\n",
    "\n",
    "99% of the time, it is recommended to scale your data so that each feature has mean 0 and standard deviation 1. See [this](http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html) for a deeper explanation.\n",
    " \n",
    "You can use scikit-learn's `scale` method from the `preprocessing` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Normalization\n",
    "\n",
    "When training a neural network using **Gradient Descent** (or any of its variants like SGD), the scale of the input features dramatically influences the optimization process and convergence speed.\n",
    "The step size of the gradient update for a weight $\\mathbf{W}_i$ is proportional to the corresponding input feature $\\mathbf{x}_i$, as the weight gradient is calculated as:\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{W}} \\propto \\mathbf{x}_{\\text{prev}}$$\n",
    "\n",
    "If feature magnitudes vary greatly (e.g., feature 1 ranges from 0 to 1, while feature 2 ranges from 1 to 1,000,000), the optimizer will struggle:\n",
    "* Features with **large magnitudes** will dominate the gradients, leading to very large steps and potentially causing the optimization to **overshoot or diverge**.\n",
    "* The optimization landscape will be highly elliptical (stretched), requiring a tiny learning rate to prevent divergence, which significantly **slows down convergence**.\n",
    "\n",
    "**Thus, it is strongly recommended to always scale your input data.**\n",
    "\n",
    "The most common and effective technique is **standardization**, which transforms the data such that each feature has a **mean of 0** and a **standard deviation of 1**:\n",
    "\n",
    "$$ \\mathbf{x}' = \\frac{\\mathbf{x} - \\mu}{\\sigma} $$\n",
    "\n",
    "This ensures that the weights for all features are updated on a similar scale, leading to a more circular optimization landscape and faster, more stable convergence.\n",
    "\n",
    "You can typically achieve this transformation using utility functions like scikit-learn's `scale` function from the `preprocessing` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# California housing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "housing = datasets.fetch_california_housing()\n",
    "print(housing[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = housing[\"data\"]\n",
    "y = housing[\"target\"]\n",
    "\n",
    "X = preprocessing.scale(X)\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y,\n",
    "    test_size=.3,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "nn = NN(\n",
    "    dimensions=(8, 10, 1),\n",
    "    activations=(ReLU, Identity),\n",
    "    loss=MSE,\n",
    "    optimizer=SGD(learning_rate=1e-3)\n",
    ")\n",
    "nn.fit(X_train, y_train, epochs=50, batch_size=8, print_every=10)\n",
    "\n",
    "y_pred = nn.predict(X_test)\n",
    "\n",
    "print(metrics.mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "X, y = datasets.load_digits(return_X_y=True)\n",
    "\n",
    "# One-hot encode y\n",
    "y = np.eye(10)[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y,\n",
    "    test_size=.3,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "nn = NN(\n",
    "    dimensions=(64, 15, 10),\n",
    "    activations=(ReLU, Sigmoid),\n",
    "    loss=MSE,\n",
    "    optimizer=SGD(learning_rate=1e-3)\n",
    ")\n",
    "nn.fit(X_train, y_train, epochs=500, batch_size=16, print_every=100)\n",
    "\n",
    "y_pred = nn.predict(X_test)\n",
    "print(metrics.classification_report(y_test.argmax(1), y_pred.argmax(1)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning framework : Pytorch\n",
    "\n",
    "[Pytorch](https://pytorch.org/) is a deep learning framework allowing to automate many operations.\n",
    "It also provides a lot of tools to create and train deep learning models.\n",
    "\n",
    "In deep learning, the type of data used is the **Tensor**, which are equivalent to numpy **ndarray**.\n",
    "A tensor is an array of data that can contain vectors, images and much more!\n",
    "\n",
    "You can instantiate a tensor using the `torch.tensor` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.0: </b>\n",
    "* Create a tensor `age_1` containing your age with type `long` and a tensor `size_1` containing your height in cm with type `float32`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_1 = ...\n",
    "size_1 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.1: </b>  \n",
    "Imagine your task is to predict the life expectancy of a person $y_i$ from a set of biological measure $(x_i^0,x_i^1,x_i^2,\\ldots,x_i^6)$.\n",
    "* Create three tensors `x_1`, `x_2` and `x_3` of biological data and form a data batch of size 3 `batch_x` with these three tensors.\n",
    "\n",
    "**Tip**: Use `torch.stack` (cf. [documentation](https://docs.pytorch.org/docs/stable/generated/torch.stack.html)).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = ...\n",
    "x2 = ...\n",
    "x3 = ...\n",
    "batch_x = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also check some attributes of your tensor.\n",
    "For example you can look at the shape of the tensor using the `shape` attribute, the gradient of a tensor using the `grad` attribute and the type using `dtype`.\n",
    "\n",
    "You can also see under which device your tensor is with the attribute `device`.\n",
    "Finally you can also put your data on gpu using the `torch.tensor.to` method.\n",
    "The possible devices are \"cpu\" and \"cuda\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.2: </b>  \n",
    "* Look at the device, gradient, type and shape of your tensors.\n",
    "* Change the device of your tensors to \"cuda\".\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "CUDA is Nvidia's library to talk with GPUs for accelerated parallel computing. If you don't have a Nvidia GPU on your computer, you will not be able to change the device of your tensors and all the neural network computation will be done on CPU, which is way slower.\n",
    "\n",
    "As we only work with tiny network in these notebooks, this will not be perceptible but when tackling real world problems, using GPU is a must-have in Deep Learning.\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we will use the California housing dataset once again.** \n",
    "\n",
    "To manage more easily the processing of your data, pytorch proposes a tool: the `Dataset` class.\n",
    "\n",
    "This class allows the creation of a data generator which will be very useful when training your model.\n",
    "\n",
    "This dataset allows to retrieve the data as a tensor.\n",
    "The dataset implements 2 methods : The `__get_item__` method which allows to access to a sample and the `__len__` method which returns the number of sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.4: </b>  \n",
    "Implement a torch dataset allowing you to access the boston dataset.\n",
    "* The `__get_item__` method should return a dict containing the data and labels at a given index in tensor form.\n",
    "* The `__len__` method should return the number of samples.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = ...\n",
    "        self.y = ...\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        return {\"data\": ..., \"label\": ...}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.5: </b>  \n",
    "Set up a dataset for the train set and one for the validation set.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ...\n",
    "val_dataset = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"First sample (x, y) of the dataset : {train_dataset[0]} \\n\") # get_item method\n",
    "print(f\"There are {len(train_dataset)} samples in the dataset.\") # len method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dataset is created, another class is used to wrap this dataset: The `Dataloader`.\n",
    "A dataloader allows for easily sampling batches of data and parallelizating the batch formation on several workers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batching\n",
    "\n",
    "At each iteration $i$, instead of updating the weights $w_i$ by using the gradient $g_i$, we can accumulate the gradients and only update the weights every $k$ iterations.\n",
    "\n",
    "The gradient we will use to update the weights will thus be the average of the past $k$ gradients. This is called **mini-batch gradient descent**. Stochastic gradient descent can be seen as a special case of mini-batch gradient descent when the batch size is set to 1.\n",
    "\n",
    "![mini-batch](mini-batch.png)\n",
    "\n",
    "The batch size is important. Small batch size work well because they are a form of regularization.\n",
    "\n",
    "Here are some links if you want to get some intuitions:\n",
    "\n",
    "- [Tradeoff batch size vs. number of iterations to train a neural network - Cross Validated](https://stats.stackexchange.com/questions/164876/tradeoff-batch-size-vs-number-of-iterations-to-train-a-neural-network)\n",
    "- [What is batch size in neural network? - Cross Validated](https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network)\n",
    "- [In deep learning, why don't we use the whole training set to compute the gradient? - Quora](https://www.quora.com/In-deep-learning-why-dont-we-use-the-whole-training-set-to-compute-the-gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.6: </b>  \n",
    "Create a train dataloader and a validation dataloader that returns sample batches of size 16.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = ...\n",
    "num_workers = ...\n",
    "train_dataloader = ...\n",
    "val_dataloader = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.7: </b>  \n",
    "* Inspect the first batch of your training loader.\n",
    "* Create a data variable and a label variable containing respectively the data and the labels of the first batch\n",
    "\n",
    "**Tip**: To get the first element of the loader use: `next(iter(loader))`\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch = ...\n",
    "data = ...\n",
    "label = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.8: </b>\n",
    "\n",
    "* Define a simple Multi-Layer Perceptron (MLP) model for the regression task. Your model should accept the 8 input features of the California Housing dataset and output a single price prediction.\n",
    "As an example, your model could have a single hidden layer containing 64 neurons.\n",
    "\n",
    "* Your model should not contain an activation function at the output of its last layer: explain why.\n",
    "\n",
    "**Hint**: use Torch layers and activations defined in `torch.nn` (cf. [documentation](https://docs.pytorch.org/docs/stable/nn.html)).\n",
    "\n",
    "<details>\n",
    "    <summary><strong>Help</strong></summary>\n",
    "    1. More specifically, use the nn.Linear layers with the nn.Sigmoid or nn.ReLU activation functions.<br>\n",
    "    2. What do you want to predict? What would adding a sigmoid activation after the last layer do?\n",
    "</details>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLPRegression(nn.Module): # All pytorch models must inherit the nn.Module\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()  # The constructor of the class calls the constructor of its parent class with the keyword \"super\".\n",
    "        self.layer1 = ...       # First layer\n",
    "        self.activation1 = ...  # Activation after first layer\n",
    "        self.layer2 = ...       # Second layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Torch modules can be called directly, exactly like functions, e.g. for a layer `my_layer`\n",
    "        # output = my_layer(input)\n",
    "        value = ...\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.9: </b>  \n",
    "Use your neural network to make a prediction on the first batch of data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = ...\n",
    "mlp_regression = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will move on to the most important part of the session: Training the model.\n",
    "Writing a **training loop** is not a simple thing at first.\n",
    "The following code implements a generic training loop that you can keep as a reference for your future work in deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.10: </b>  \n",
    "* Explain the role of each argument of the function\n",
    "* Fill in the blank code\n",
    "* Comment on each line of the training loop (except the lines used for visualization)\n",
    "* Raise your hand to give me an oral report on this question \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "def train_regressor(\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    train_loader: DataLoader,\n",
    "    valid_loader: DataLoader,\n",
    "    nb_epoch: int,\n",
    "    criterion: nn.Module,\n",
    "    batch_size: int=16,\n",
    "    device: torch.device = torch.device(\"cuda:0\"),\n",
    "    verbose: bool=True,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Pytorch training loop\n",
    "    Args:\n",
    "        model (nn.Module): Pytorch classification model\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for the model\n",
    "        train_loader (DataLoader): DataLoader for training fold\n",
    "        valid_loader (DataLoader): DataLoader for validation fold\n",
    "        nb_epoch (int): Number of epoch\n",
    "        criterion (nn.Module): Loss\n",
    "        device (torch.device): .Defaults to `torch.device(\"cuda:0\")`\n",
    "        verbose (bool): Verbose term\n",
    "    \"\"\"\n",
    "    loaders = {\"train\": train_loader, \"validation\": valid_loader}\n",
    "    model.to(device)\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for epoch in range(1, nb_epoch + 1):\n",
    "        if verbose:\n",
    "            print(\"-\" * 80)\n",
    "        for phase in [\"train\", \"validation\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            for sample in loaders[phase]:\n",
    "                data = ...\n",
    "                label = ...\n",
    "                optimizer.zero_grad()\n",
    "                data, label = data.to(device), label.to(device)\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    output = ...\n",
    "                    pred_label = ...\n",
    "                    loss = ...\n",
    "                    if phase == \"train\":\n",
    "                        ... # Compute the gradient\n",
    "                        ... # Make a gradient step\n",
    "                running_loss += loss.item()\n",
    "            epoch_loss = running_loss / (len(loaders[phase].dataset)/batch_size)\n",
    "            if phase == \"train\":\n",
    "              train_loss.append(epoch_loss)\n",
    "            else:\n",
    "              val_loss.append(epoch_loss)\n",
    "            if verbose:\n",
    "              print(\n",
    "                  f\" Epoch number: {epoch}, Phase: {phase}, Loss value: {epoch_loss:.4f}\"\n",
    "              )\n",
    "\n",
    "    fig, (axe1, axe2) = plt.subplots(2, figsize=(10, 10))\n",
    "    fig.suptitle('Training and validation statistics')\n",
    "\n",
    "    y_pred = model(torch.from_numpy(X_val).to(device)).detach().cpu().numpy()\n",
    "    axe1.plot(np.arange(len(train_loss)), train_loss,label=\"train MSE\")\n",
    "    axe1.plot(np.arange(len(val_loss)), val_loss,label = \"val MSE\")\n",
    "    axe1.set_title(\"MSE loss\")\n",
    "    axe1.legend()\n",
    "\n",
    "    sort_idx = y_val.argsort(axis=0).squeeze()\n",
    "    axe2.scatter(range(len(y_val)), y_val[sort_idx], label='target', zorder=2)\n",
    "    axe2.scatter(range(len(y_val)), y_pred[sort_idx], label='prediction')\n",
    "    axe2.set_title(f\"Prediction // MSE = {mean_absolute_error(y_pred, y_val)}\")\n",
    "    axe2.legend()\n",
    "\n",
    "    plt.show()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.11: </b>  \n",
    "Instantiate the list of parameters necessary to run the function and justify each of these parameters (loss, optimizer, learning rate, network architecture...)\n",
    "</div>\n",
    "\n",
    "**Tip**: You can use the `Adam` optimizer and get inspiration from https://pytorch.org/docs/stable/nn.html#loss-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam, SGD\n",
    "\n",
    "mlp_logistic = ...\n",
    "batch_size = ...\n",
    "num_workers = ...\n",
    "train_dataloader = ...\n",
    "val_dataloader = ...\n",
    "learning_rate = ...\n",
    "optimizer = ...\n",
    "criterion = ...\n",
    "device = ...\n",
    "nb_epochs = ...\n",
    "verbose = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, val_loss = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.12: </b>  \n",
    "Comment the results on : \n",
    "* The loss function\n",
    "* The model error\n",
    "\n",
    "Can we do better and how?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.13: </b>  \n",
    "* Implement a new deeper architecture.\n",
    "* Set the necessary parameters for the training again and restart the procedure.\n",
    "* Comment on the new results obtained\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLPDeep(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        ...\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_deep = ...\n",
    "batch_size = ...\n",
    "num_workers = ...\n",
    "train_dataloader = ...\n",
    "val_dataloader = ...\n",
    "learning_rate = ...\n",
    "optimizer = ...\n",
    "criterion = ...\n",
    "device = ...\n",
    "nb_epochs = ...\n",
    "verbose = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, val_loss = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.14: </b>  \n",
    "* Train the model for 100 epochs and comment on the results obtained.\n",
    "* Plot the loss function and the model error as a function of the epochs for the training and validation sets.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The phenomena of exercise 1.14 is called overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b> Exercise 1.15: </b>   \n",
    "* Explain in your own words what is overfitting.\n",
    "* How can we avoid this phenomenon?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.16: </b>  \n",
    "* Implement early stopping in the training loop.\n",
    "* Add some regularization \n",
    "* Add weight decay on the optimizer and explain in your own words what is weight decay.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "In the previous cells we just mentionned the word **regularization**, what is it exactly?\n",
    "\n",
    "1. A machine learning model learns on a training set \n",
    "2. We want the model to perform well on a test set it hasn't seen\n",
    "3. The deeper the model, the greater the chance that it overfits by memorizing patterns that only exist in the training set\n",
    "4. Regularizing a model means that we make it's life harder  \n",
    "5. There are many ways to regularize a neural network:\n",
    "    1. Use more data! The more training data there is, the more the model will focus on general patterns\n",
    "    2. Penalize the updates made to the weights by the optimizer \n",
    "    3. Use dropout\n",
    "    4. Use batch normalization\n",
    "    5. Use early stopping\n",
    "    \n",
    "![complexity](complexity.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to choose the architecture of the neural network ?\n",
    "\n",
    "There are so many choices you can make that it can quickly become overwhelming. Choosing the right pieces of the puzzle is very much an art rather than a science. There is no getting around trying things out. It is thus very important to setup a stable environment for testing and evaluating model choices. You should always start by defining a reliable testing procedure.\n",
    "\n",
    "Here is some general advice:\n",
    "\n",
    "1. Start with a very simple model (for example a logistic regression)\n",
    "2. Add complexity to the model as long as the validation score improves\n",
    "3. If the model is overfitting (you can detect it by comparing the training and validation scores) then add regularization\n",
    "4. Last but not least, spend most of your time checking that the data you're using is correct\n",
    "\n",
    "Here are some more links if you're interested:\n",
    "\n",
    "- [How to decide neural network architecture? - Data Science exchange](https://datascience.stackexchange.com/questions/20222/how-to-decide-neural-network-architecture)\n",
    "- [How to choose the number of hidden layers and nodes in a feedforward neural network? - Cross Validated](https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
